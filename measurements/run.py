"""Script to run the measurements necessary for evaluation
"""

import csv
import functools
import os
import random
import re
import subprocess
from math import ceil
from math import log as ln

import pandas as pd
import tabulate
from config import (
    JoinConfig,
    PrjParam,
    backup_cpu_mapping,
    get_cpu_thread_step_config,
    get_static_conf,
    restore_cpu_mapping,
    set_cpu_mapping,
    set_prj_params,
    set_cpu_constant
)

debug_flag = False
path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
cwd = f"cd {path}/src; "
result_path_pkl = f"{os.path.dirname(os.path.abspath(__file__))}/data/pkl"
result_path_md = f"{os.path.dirname(os.path.abspath(__file__))}/data/md"


def set_debug(enable):
    global debug_flag
    debug_flag = enable


def debug(msg):
    if debug_flag:
        print(msg)


def log(msg):
    print(msg)


def run_configure(args=[]):
    """Run the configure command appended with the provided args and regenerate the binaries

    Args:
        args (list, optional): arguments to `./configure`
    """
    command = f"cd {path}; ./configure"
    for arg in args:
        command += " " + arg

    command += "; make clean; make"

    try:
        p = subprocess.run(command, capture_output=True, check=True, shell=True)
        debug("Configuration completed successfully\n")
        debug(p.stdout.decode())
        debug(p.stderr.decode())
    except subprocess.CalledProcessError as e:
        print(f"Failed to configure project ({e.returncode}):\n", e)
        exit(1)


def run_measurement(config: JoinConfig) -> dict:
    """Runs a specific measurement given the provided config using ./mchashjoins

    Args:
        config (JoinConfig): Config for ./mchashjoins exec.

    Returns:
        dict: the collected data
    """
    command = f"./mchashjoins {config.getArgsString()}"

    try:
        log(f'Executing "{command}"...')
        p = subprocess.run(cwd + command, capture_output=True, check=True, shell=True)
        log(" Ok.\n")
        output = p.stdout.decode()
        debug(output)
        debug(p.stderr.decode())
        return {
            **config.toDict(),
            **parse_result(output),
        }
    except subprocess.CalledProcessError as e:
        print(f"Measurement failed ({e.returncode}):\n", e)
        print(e.stdout)
        print(e.stderr)

    return {**config.toDict()}


def parse_result(res: str) -> dict:
    """Parse output generated by ./mchashjoins as a dictionary

    Args:
        res (str): textual output

    Returns:
        dict: parsed data as a structured dict
    """
    s_size = int(
        re.search("relation S with size = [\d.]+ MiB, #tuples = (\d+) : OK", res).group(
            1
        )
    )
    filtered = re.search("S-tuples after filter: (\d+)\n", res)
    if filtered is not None:
        filtered = filtered.group(1)
    else:
        filtered = None
    [runtime_cycles, build_cycles, part_cycles] = re.search(
        "RUNTIME TOTAL, BUILD, PART \(cycles\):\W+(\d+)\W+(\d+)\W+(\d+)", res
    ).groups()
    [time_usecs, out_tuples, cycles_per_tuple] = re.search(
        "TOTAL-TIME-USECS, TOTAL-TUPLES, NSEC-PER-TUPLE:\W+([\d.]+)\W+(\d+)\W+([\d.]+)",
        res,
    ).groups()
    [partition_usecs, probe_usecs, join_usecs] = re.search(
        "PARTITION-TIME-USECS, PROBE-TIME-USECS, JOIN-TIME-USECS:\W+([\d.]+)\W+([\d.]+)\W+([\d.]+)",
        res,
    ).groups()

    data = {
        "filtered": int(filtered) if filtered else None,
        "filtered-pct": int(filtered) / s_size * 100 if filtered else None,
        "runtime-cycles": int(runtime_cycles),
        "build-cycles": int(build_cycles),
        "part-cycles": int(part_cycles),
        "time-usecs": float(time_usecs),
        "out-tuples": int(out_tuples),
        "nsec-per-tuple": float(cycles_per_tuple),
        "partition-usecs": float(partition_usecs),
        "probe-usecs": float(probe_usecs),
        "join-usecs": float(join_usecs),
    }

    if "Counter" in res:
        for i in range(15):
            [value, name] = re.search(
                f"Counter {i} = ([^\s]+) \(([^\)]+)\)", res
            ).groups()
            try:
                value = float(value)
            except:
                value = None
            data[name] = value

    return data


def best_hash_function(seed=1337):
    """Tests the performance of selected hash functions on the current machine and
    ranks them by speed and collisions.
    This should show that CRC and CrapWow are performing best or at least comparable to
    the best 2 hash functions. If not, evaluation should make use of the best hash
    functions, which requires manual action in the `bloom_filter.c` code.
    This function prints details about:

    1. speed (per hash)
    1. number of collisions
    """
    n_samples = 100000000
    random.seed(seed)

    try:
        seed = random.randint(0, 2147483647)
        command = f"./unittests 0 {seed} {n_samples}"
        log(command)
        p = subprocess.run(cwd + command, capture_output=True, check=True, shell=True)
        log(" Ok.\n")
        output = p.stdout.decode()
        debug(output)
        debug(p.stderr.decode())
        reader = csv.reader(output.split("\n")[:-1], delimiter=";")
        header = reader.__next__()
        data = [
            [x[0], float(x[1]), float(x[2]), int(x[3]), float(x[4])] for x in reader
        ]

        def compare(x, y):
            if x[4] * 2 < y[4]:
                return -1
            if y[4] * 2 < x[4]:
                return 1
            return x[2] - y[2]

        data = sorted(data, key=functools.cmp_to_key(compare))
        data.insert(0, header)
        print(tabulate.tabulate(data, headers="firstrow"))
        return pd.DataFrame(data[1:], columns=header)
    except subprocess.CalledProcessError as e:
        print(f"Measurement failed ({e.returncode}):\n", e)
        print(e.stdout)
        print(e.stderr)


def never_single_pass():
    """Theoretically, single pass radix join can never profit from the bloom filter
    because of the number of accesses and their access patterns (see thesis).
    This function provides supporting indications that back up this theoretical
    finding. It is by far not exhaustive but validates the claim by a number of
    experiments that cover a broad range of different parameters.
    """
    cpu_mapping_initial = backup_cpu_mapping()

    config = JoinConfig(
        r_size=32000000,
        s_size=1024000000,
        bloom_block_size=512,
        bloom_size=1 << 28,
    )

    results = []
    for s_sel in [0.01, 0.1]:
        config.s_sel = s_sel
        for nthreads, _, mapping in get_cpu_thread_step_config():
            set_cpu_mapping(mapping)
            config.nthreads = nthreads

            config.algo = "NPO"
            no_partition = run_measurement(config)
            results += [no_partition]
            config.algo = None

            results += [{"algo": tabulate.SEPARATING_LINE}]

            for num_passes in [1, 2]:
                prj_params = [PrjParam("NUM_PASSES", num_passes)]
                old_prj_params = set_prj_params(prj_params)
                run_configure()

                config.bloom_filter = "no"
                config.bloom_hashes = 0
                unfiltered = run_measurement(config)
                tmp_res = [unfiltered]
                for k in [1, 2, 3]:
                    config.bloom_hashes = k

                    config.bloom_filter = "basic"
                    basic = run_measurement(config)

                    config.bloom_filter = "blocked"
                    blocked = run_measurement(config)

                    tmp_res += [basic, blocked]

                static_conf = get_static_conf()
                results += [{**static_conf, **x} for x in tmp_res]
                results += [{"algo": tabulate.SEPARATING_LINE}]
                set_prj_params(old_prj_params, reset=True)

            results += [{"algo": tabulate.SEPARATING_LINE}]
        results += [{"algo": tabulate.SEPARATING_LINE}]

    print(tabulate.tabulate(results, headers="keys", showindex=True))
    restore_cpu_mapping(cpu_mapping_initial)
    run_configure()

    return pd.DataFrame(
        [x for x in results if tabulate.SEPARATING_LINE not in x.values()]
    )


def best_bloom_filter_type():
    """This function shows that a blocked bloom filter will always perform better
    than a basic bloom filter if it the filter does not fit in the cache and k > 1.

    This shows the necessecity of choosing a blocked bloom filter in most cases.
    If the filter does not fit in the cache, filter accesses to the basic filter
    result in a cache miss most of the time, while there will be at most 1 cache
    miss per tuple for the blocked variant. This is because all bits are stored in a
    single cache line, resulting in cache hits for subsequent accesses to the same
    block.
    This claim is only true for mostly uncached filters and k > 1.

    This test is by far not exhaustive but validates the claim by a number of
    experiments that cover a broad range of different parameters.
    """
    cpu_mapping_initial = backup_cpu_mapping()

    config = JoinConfig(s_sel=0.01, bloom_block_size=512)

    results = []
    for r, m in [(250000, 2097152), (16000000, 134217728), (128000000, 1073741824)]:
        config.r_size = r
        config.s_size = r * 8
        config.bloom_size = m

        for nthreads, _, mapping in get_cpu_thread_step_config():
            set_cpu_mapping(mapping)
            config.nthreads = nthreads

            tmp_res = []

            for k in range(1, 8):
                config.bloom_hashes = k

                config.bloom_filter = "basic"
                basic = run_measurement(config)

                config.bloom_filter = "blocked"
                blocked = run_measurement(config)

                tmp_res += [basic, blocked]

            static_conf = get_static_conf()
            results += [{**static_conf, **x} for x in tmp_res]

        results += [{"cpu-mapping": tabulate.SEPARATING_LINE}]

    print(tabulate.tabulate(results, headers="keys", showindex=True))
    restore_cpu_mapping(cpu_mapping_initial)
    return pd.DataFrame(
        [x for x in results if tabulate.SEPARATING_LINE not in x.values()]
    )


def test_parameters():
    """Tests the effects of various parameters on the performance of
    the bloom filter and the radix join.

    This function runs exhaustive tests to provide raw data for analysis of the
    performance. Depending on configuration and underlying hardware, these
    tests might require long running times as the search space is quite large.
    This function returns a pandas dataframe with the collected data and the
    corrensponding configurations that were the base of each run.
    """
    config = JoinConfig()
    config.bloom_block_size = 512
    all_results = []
    for max_threads, step, mapping in get_cpu_thread_step_config():
        results = []
        set_cpu_mapping(mapping)
        for r_size in [250000, 16000000, 128000000]:
            config.r_size = r_size
            for r_s_ratio in [0.125, 0.25, 1]:
                s_size = int(r_size / r_s_ratio)
                config.s_size = s_size
                for s_sel in [0.001, 0.01, 0.1]:
                    config.s_sel = s_sel
                    for nthreads in range(step, max_threads + 1, step):
                        config.nthreads = nthreads

                        config.bloom_filter = "no"
                        results += [run_measurement(config)]
                        for bits_per_key in [4, 8, 12]:
                            # m = next power of 2
                            m = 1 << (bits_per_key * r_size - 1).bit_length()
                            config.bloom_size = m
                            k_opt = ceil(m / r_size * ln(2))

                            for k in range(1, max(2, k_opt + 1)):
                                config.bloom_hashes = k
                                config.bloom_filter = "blocked" if k > 2 else "basic"
                                results += [run_measurement(config)]

        for res in results:
            res["cpu-mapping"] = mapping

        print(tabulate.tabulate(results, headers="keys"))
        all_results += results

    data = pd.DataFrame(all_results)
    data = data.round({"filtered-pct": 2})
    return data


def cross_run():
    """Run selected scenarios with enabled performance counters to measure cache and TLB behavior"""
    config = JoinConfig()
    all_res = []

    run_configure(args=["--enable-perfcounters"])

    for mapping, s_sel, r, s, bloom_filter, bloom_hashes, bloom_size in [
        ("hypthr", 0.001, 16, 128, "basic", 1, 1 << 27),  # gondor
        ("numa", 0.01, 16, 128, "basic", 1, 1 << 27),  # celebrimbor
        ("all", 0.1, 16, 128, "basic", 1, 1 << 26),  # isengard
        ("all", 0.001, 128, 1024, "blocked", 3, 1 << 30),  # forostar
        ("single", 0.01, 128, 512, "basic", 1, 1 << 30),  # mittalmar
    ]:
        nthreads = [
            nthr for nthr, step, map in get_cpu_thread_step_config() if map == mapping
        ][0]
        config.nthreads = nthreads
        config.s_sel = s_sel
        config.bloom_size = bloom_size
        config.r_size = r * 1000000
        config.s_size = s * 1000000
        config.bloom_filter = bloom_filter
        config.bloom_hashes = bloom_hashes
        set_cpu_mapping(mapping)
        res = run_measurement(config)
        res["cpu-mapping"] = mapping
        all_res += [res]

        config.bloom_filter = "no"
        res = run_measurement(config)
        res["cpu-mapping"] = mapping
        all_res += [res]

    save_data(pd.DataFrame(all_res), "cross_run")
    run_configure()


def save_data(df: pd.DataFrame, filename: str):
    """Save data as pickle to `data/pkl/<filename>.pkl` and if available as markdown to `data/md/<filename>.md`

    Args:
        df (pd.DataFrame): data to save
        filename (str): filename to save to
    """
    df.to_pickle(f"{result_path_pkl}/{filename}.pkl")
    try:
        df.to_markdown(f"{result_path_md}/{filename}.md")
    except AttributeError:
        pass


if __name__ == "__main__":
    set_debug(False)
    os.makedirs(result_path_pkl, exist_ok=True)
    os.makedirs(result_path_md, exist_ok=True)
    set_cpu_constant()
    run_configure()

    cross_run()

    data = best_hash_function()
    save_data(data, "hash_functions")

    data = never_single_pass()
    save_data(data, "single_vs_multi_pass")

    data = best_bloom_filter_type()
    save_data(data, "basic_vs_blocked")

    data = test_parameters()
    save_data(data, "test_parameters")

    if not hasattr(data, "to_markdown"):
        print(
            "[WARN] Unable to create markdown files because of pandas version. Pickles are created anyways."
        )
    print("done")
